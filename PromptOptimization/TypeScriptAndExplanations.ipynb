{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Problem\n",
    "\n",
    "Note: I initially did all of my tests using Ollama models (mistral:7b, gemma3:4b, and qwen3:8b), however, I lost a lot of time trying to setup the structured json outputs for these small models. Due to this I ended up using OpenAI gpt-4o-mini instead to avoid dealing with parsing issues.\n",
    "\n",
    "For the project I in fact used 4 datasets (PiQA, HellaSwag, BoolQ, and GMS8K)\n",
    "The reason for this is actually because I originally intended to use a drug-drug interaction task using DrugBank's API, however the data engineering ended up a bit too complicated.\n",
    "So instead (as said in the project guidelines) I used some open source datasets.\n",
    "\n",
    "The primary dataset that I wanted to analyse the results over is HellaSwag which is a dataset of common-sense reasoning MCQ, however, I also wanted to have other reasoning datasets in order to cross-compare the different performances. I also felt only doing one of these reasoning tasks would not be sufficient enough given the scope of the mini project. This ended up having some interesting results, so I'm glad that I did (albeit the extra time and costs).\n",
    "\n",
    "## PiQA :\n",
    "- Problem: This task evaluates physical commonsense reasoning. The model is given a \"goal\" and must choose which of two possible solutions is the more practical or correct one.\n",
    "## HellaSwag:\n",
    "- Problem: This is a commonsense Natural Language Inference (NLI) task. The model is given a context (the beginning of a sentence) and must choose the most plausible ending from four options\n",
    "## BoolQ:\n",
    "- Problem: This is a reading comprehension task requiring a \"yes\" or \"no\" answer. The model is given a passage of text and a question about it.\n",
    "  \n",
    "and lastly what the model(s) should do well against\n",
    "## GMS8K:\n",
    "- Problem: This task evaluates the model's ability to perform multi-step mathematical reasoning to solve grade-school-level word problems.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial test prompt(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Initial test prompts per task\n",
    "const INSTRUCTIONS = {\n",
    "  piqa:\n",
    "    'You are a classifier for physical commonsense reasoning. ' +\n",
    "    'Analyze both options carefully and choose the more practical solution. ' +\n",
    "    'Return ONLY valid JSON: {\"label\": \"A\"} or {\"label\": \"B\"}.',\n",
    "\n",
    "  hellaswag:\n",
    "    'You are a commonsense reasoning classifier. ' +\n",
    "    'Read the context and determine which ending makes most sense. ' +\n",
    "    'Return ONLY valid JSON: {\"label\": \"A|B|C|D\"}.',\n",
    "\n",
    "  boolq:\n",
    "    'You are a reading comprehension system. ' +\n",
    "    'Read the passage and answer the question truthfully. ' +\n",
    "    'Return ONLY valid JSON: {\"answer\": \"yes\"} or {\"answer\": \"no\"}.',\n",
    "\n",
    "  gsm8k:\n",
    "    'You are a math problem solver. ' +\n",
    "    'Solve the problem step by step. ' +\n",
    "    'Return ONLY valid JSON: {\"answer\": <number>, \"reasoning\": \"<optional>\"}.'\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HellaSwag loaded: \u001b[33m8\u001b[39m examples\n",
      "Sample: {\n",
      "  question: \u001b[32m\"A man is sitting on a roof. he\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"is using wrap to wrap a pair of skis.\"\u001b[39m,\n",
      "    \u001b[32m\"is ripping level tiles off.\"\u001b[39m,\n",
      "    \u001b[32m\"is holding a rubik's cube.\"\u001b[39m,\n",
      "    \u001b[32m\"starts pulling up roofing on a roof.\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"D\"\u001b[39m\n",
      "}\n",
      "Sample: {\n",
      "  question: \u001b[32m\"A man is sitting on a roof. he\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"is using wrap to wrap a pair of skis.\"\u001b[39m,\n",
      "    \u001b[32m\"is ripping level tiles off.\"\u001b[39m,\n",
      "    \u001b[32m\"is holding a rubik's cube.\"\u001b[39m,\n",
      "    \u001b[32m\"starts pulling up roofing on a roof.\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"D\"\u001b[39m\n",
      "}\n",
      "Sample: {\n",
      "  question: \u001b[32m\"A lady walks to a barbell. She bends down and grabs the pole. the lady\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"swings and lands in her arms.\"\u001b[39m,\n",
      "    \u001b[32m\"pulls the barbell forward.\"\u001b[39m,\n",
      "    \u001b[32m\"pulls a rope attached to the barbell.\"\u001b[39m,\n",
      "    \u001b[32m\"stands and lifts the weight over her head.\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"D\"\u001b[39m\n",
      "}\n",
      "Sample: {\n",
      "  question: \u001b[32m\"Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. the child and a different man\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"are then shown paddling down a river in a boat while a woman talks.\"\u001b[39m,\n",
      "    \u001b[32m\"are driving the canoe, they go down the river flowing side to side.\"\u001b[39m,\n",
      "    \u001b[32m\"sit in a canoe while the man paddles.\"\u001b[39m,\n",
      "    \u001b[32m\"walking go down the rapids, while the man in his helicopter almost falls and goes out of canoehood.\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"C\"\u001b[39m\n",
      "}\n",
      "Sample: {\n",
      "  question: \u001b[32m\"A boy is running down a track. the boy\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"runs into a car.\"\u001b[39m,\n",
      "    \u001b[32m\"gets in a mat.\"\u001b[39m,\n",
      "    \u001b[32m\"lifts his body above the height of a pole.\"\u001b[39m,\n",
      "    \u001b[32m\"stands on his hands and springs.\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"C\"\u001b[39m\n",
      "}\n",
      "Sample: {\n",
      "  question: \u001b[32m\"The boy lifts his body above the height of a pole. The boy lands on his back on to a red mat. the boy\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"turns his body around on the mat.\"\u001b[39m,\n",
      "    \u001b[32m\"gets up from the mat.\"\u001b[39m,\n",
      "    \u001b[32m\"continues to lift his body over the pole.\"\u001b[39m,\n",
      "    \u001b[32m\"wiggles out of the mat.\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"B\"\u001b[39m\n",
      "}\n",
      "Sample: {\n",
      "  question: \u001b[32m\"The boy lands on his back on to a red mat. The boy gets up from the mat. the boy\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"starts doing spins.\"\u001b[39m,\n",
      "    \u001b[32m\"celebrates by clapping and flexing both arms.\"\u001b[39m,\n",
      "    \u001b[32m\"is dancing on the mat.\"\u001b[39m,\n",
      "    \u001b[32m\"does jump jacks on his stick.\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"B\"\u001b[39m\n",
      "}\n",
      "Sample: {\n",
      "  question: \u001b[32m\"A man is standing in front of a camera. He starts playing a harmonica for the camera. he\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"begins to play the harmonica with his body while looking at the camera.\"\u001b[39m,\n",
      "    \u001b[32m\"seems to be singing while playing the harmonica.\"\u001b[39m,\n",
      "    \u001b[32m\"rocks back and forth to the music as he goes.\"\u001b[39m,\n",
      "    \u001b[32m\"painted a fence in front of the camera.\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"C\"\u001b[39m\n",
      "}\n",
      "Sample: {\n",
      "  question: \u001b[32m\"A cartoon animation video is shown with people wandering around and rockets being shot. two men\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"fight robots of evil and ends with a to be continued.\"\u001b[39m,\n",
      "    \u001b[32m\"are then shown in closeups shooting a shot put.\"\u001b[39m,\n",
      "    \u001b[32m\"push a child in a speedboat in the water.\"\u001b[39m,\n",
      "    \u001b[32m\"look in the cameraman's eye and smile.\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"A\"\u001b[39m\n",
      "}\n",
      "Sample: \u001b[90mundefined\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "// HellaSwag\n",
    "import {loadHellaSwag} from \"./datasets.ts\";\n",
    "\n",
    "const hellaData = await loadHellaSwag(8);\n",
    "console.log(\"HellaSwag loaded:\", hellaData.length, \"examples\");\n",
    "console.log(\"Sample:\", hellaData[0]);\n",
    "console.log(\"Sample:\", hellaData[0]);\n",
    "console.log(\"Sample:\", hellaData[1]);\n",
    "console.log(\"Sample:\", hellaData[2]);\n",
    "console.log(\"Sample:\", hellaData[3]);\n",
    "console.log(\"Sample:\", hellaData[4]);\n",
    "console.log(\"Sample:\", hellaData[5]);\n",
    "console.log(\"Sample:\", hellaData[6]);\n",
    "console.log(\"Sample:\", hellaData[7]);\n",
    "console.log(\"Sample:\", hellaData[8]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// PiQA\n",
    "{\n",
    "    \"goal\": \"How do I ready a guinea pig cage for it's new occupants?\",\n",
    "    \"sol1\": \"Provide the guinea pig with a cage full of a few inches of bedding made of ripped paper strips, you will also need to supply it with a water bottle and a food dish.\",\n",
    "    \"sol2\": \"Provide the guinea pig with a cage full of a few inches of bedding made of ripped jeans material, you will also need to supply it with a water bottle and a food dish.\",\n",
    "    \"label\": \"0\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// BoolQ\n",
    "{\n",
    "    \"question\": \"does ethanol take more energy make that produces\",\n",
    "    \"passage\": \"All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\",\n",
    "    \"answer\": false\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// GMS8K\n",
    "{\n",
    "    \"question\": \"Janet\\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
    "    \"answer\": 18.0,\n",
    "    \"solution\": \"Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer\\u2019s market.\\n#### 18\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator(s)\n",
    "\n",
    "For each of these datasets there is only one evaluator function which does exact match only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// For 2 or 4 choice multiple choice tasks (PiQA and HellaSwag)\n",
    "export function makeMCEvaluator(\n",
    "  config: LLMConfig,\n",
    "  numChoices: 2 | 4 = 2\n",
    "): EvalExample<MCExample> {\n",
    "  return async (instruction: string, ex: MCExample) => {\n",
    "    const choiceLabels = numChoices === 2 ? ['A', 'B'] : ['A', 'B', 'C', 'D'];\n",
    "    const optionsText = ex.choices\n",
    "      .slice(0, numChoices)\n",
    "      .map((choice, i) => `${choiceLabels[i]}) ${choice}`)\n",
    "      .join('\\n');\n",
    "\n",
    "    const user =\n",
    "      `Question: ${ex.question}\\n\\n` +\n",
    "      `Options:\\n${optionsText}\\n\\n` +\n",
    "      `Respond with ONLY the letter of the correct answer (${choiceLabels.join(' or ')}). No explanation.`;\n",
    "\n",
    "    const { response, tokens } = await chatText(\n",
    "      config,\n",
    "      [\n",
    "        { role: \"system\", content: instruction },\n",
    "        { role: \"user\", content: user }\n",
    "      ]\n",
    "    );\n",
    "\n",
    "    // Extract first uppercase letter from response\n",
    "    const match = response.match(/[A-D]/);\n",
    "    const predicted = match ? match[0] : null;\n",
    "\n",
    "    // If matched yay, else nay\n",
    "    const score = predicted === ex.correct ? 1.0 : 0.0;\n",
    "\n",
    "    return { score, tokens };\n",
    "  };\n",
    "}\n",
    "\n",
    "// Evals for PIQA and HellaSwag (2 and 4 choices)\n",
    "export function makePIQAEvaluator(config: LLMConfig): EvalExample<MCExample> {\n",
    "  return makeMCEvaluator(config, 2);\n",
    "}\n",
    "\n",
    "export function makeHellaSwagEvaluator(config: LLMConfig): EvalExample<MCExample> {\n",
    "  return makeMCEvaluator(config, 4);\n",
    "}\n",
    "\n",
    "// Boolean evaluator for BoolQ\n",
    "export function makeBoolQEvaluator(config: LLMConfig): EvalExample<BoolQExample> {\n",
    "  return async (instruction: string, ex: BoolQExample) => {\n",
    "    const user =\n",
    "      `Passage: ${ex.passage}\\n\\n` +\n",
    "      `Question: ${ex.question}\\n\\n` +\n",
    "      `Respond with ONLY \"yes\" or \"no\". No explanation.`;\n",
    "\n",
    "    const { response, tokens } = await chatText(\n",
    "      config,\n",
    "      [\n",
    "        { role: \"system\", content: instruction },\n",
    "        { role: \"user\", content: user }\n",
    "      ]\n",
    "    );\n",
    "\n",
    "    const cleaned = response.trim().toLowerCase();\n",
    "    const predicted = cleaned.includes('yes') ? 'yes' : \n",
    "                     cleaned.includes('no') ? 'no' : null;\n",
    "        \n",
    "    // If matched yay, else nay\n",
    "    const goldAnswer = ex.answer ? \"yes\" : \"no\";\n",
    "    const score = predicted === goldAnswer ? 1.0 : 0.0;\n",
    "\n",
    "    return { score, tokens };\n",
    "  };\n",
    "}\n",
    "\n",
    "// Numeric answer evaluator for GSM8K\n",
    "export function makeGSM8KEvaluator(config: LLMConfig): EvalExample<GSM8KExample> {\n",
    "  return async (instruction: string, ex: GSM8KExample) => {\n",
    "    const user =\n",
    "      `${ex.question}\\n\\n` +\n",
    "      `Respond with ONLY the final numerical answer. No explanation.`;\n",
    "\n",
    "    const { response, tokens } = await chatText(\n",
    "      config,\n",
    "      [\n",
    "        { role: \"system\", content: instruction },\n",
    "        { role: \"user\", content: user }\n",
    "      ]\n",
    "    );\n",
    "\n",
    "    // Extract last number from response\n",
    "    const numbers = response.match(/-?\\d+\\.?\\d*/g);\n",
    "    const predicted = numbers ? parseFloat(numbers[numbers.length - 1]) : null;\n",
    "    \n",
    "    const tolerance = 0.01;\n",
    "    \n",
    "    // If matched yay, else nay (w/ FP)\n",
    "    const score =\n",
    "      predicted !== null && Math.abs(predicted - ex.answer) < tolerance\n",
    "        ? 1.0\n",
    "        : 0.0;\n",
    "\n",
    "    return { score, tokens };\n",
    "  };\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Algorithms\n",
    "\n",
    "As I implemented the algorithms in typescript. Instead of running I am simply calling them from their respective modules along with the evaluator(s).\n",
    "But for reference here are the defined algorithms from the modules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"npm:zod\";\n",
    "import { chatText, LLMConfig } from \"./callOllama.ts\";\n",
    "\n",
    "// prompt is id + the prompt's text\n",
    "export type Prompt = { id: string; instruction: string };\n",
    "\n",
    "// Track best current prompt, its score, and tokens used\n",
    "export type BestPrompt = { best: Prompt; score: number; tokens: number };\n",
    "\n",
    "// Evaluator: given an instruction and an example, return scalar score in [0,1] and tokens used.\n",
    "export type EvalExample<E> = (\n",
    "  instruction: string,\n",
    "  ex: E\n",
    ") => Promise<{ score: number; tokens: number }>;\n",
    "\n",
    "// Track tokens used for budget\n",
    "export class TokenMeter {\n",
    "  total = 0;\n",
    "  add(n: number) { this.total += Math.max(0, n|0); }\n",
    "  snapshot() { return this.total; }\n",
    "}\n",
    "\n",
    "// UUID for unique prompt IDs to track\n",
    "function uuid() { return crypto.randomUUID(); }\n",
    "\n",
    "// Schema of a paraphrased prompt\n",
    "const ParaphraseSchema = z.object({\n",
    "  instruction: z.string().min(8).max(8000),\n",
    "});\n",
    "\n",
    "/**\n",
    " * Paraphrase the given prompt\n",
    " * Counts tokens used by the paraphrasing call\n",
    " */\n",
    "export async function paraphraseInstruction(\n",
    "  config: LLMConfig,\n",
    "  base: string,\n",
    "  meter: TokenMeter,\n",
    "  style = \"Be concise. Keep the same schema. Emphasize: return ONLY valid JSON.\"\n",
    "): Promise<{ instruction: string; tokens: number }> {\n",
    "  const system =\n",
    "    \"You rewrite prompts. Preserve intent and constraints; reduce verbosity; DO NOT change the output schema.\";\n",
    "  const user =\n",
    "    `Rewrite this instruction with the same meaning and constraints. Do not add examples.\\n` +\n",
    "    `STYLE: ${style}\\n\\n---\\n${base}\\n---\\n` +\n",
    "    `Return JSON: {\"instruction\": \"<rewritten>\"} (no extra fields).`;\n",
    "\n",
    "  const { response, tokens } = await chatText(\n",
    "    config,\n",
    "    [\n",
    "      { role: \"system\", content: system },\n",
    "      { role: \"user\", content: user },\n",
    "    ],\n",
    "  );\n",
    "\n",
    "  const rewritten = response.trim() || base;\n",
    "\n",
    "  meter.add(tokens ?? 0);\n",
    "  return { instruction: rewritten, tokens: tokens ?? 0 };\n",
    "}\n",
    "\n",
    "export type APEOptions<E> = {\n",
    "  config: LLMConfig; // LLM config for paraphrasing\n",
    "  baseInstruction: string; // starting instruction to paraphrase\n",
    "  N: number; // number of paraphrases to try\n",
    "  data: E[]; // data is the eval examples for: PIQA, HellaSwag, BoolQ, GSM8K\n",
    "  evalExample: EvalExample<E>; // evaluator function for the dataset\n",
    "};\n",
    "\n",
    "export async function apeOptimize<E>(opts: APEOptions<E>) {\n",
    "  const { config, baseInstruction, N, data, evalExample } = opts;\n",
    "  const meter = new TokenMeter();\n",
    "\n",
    "  // Create candidates: base + N paraphrases and set uuids + track total tokens (max per prompt is 128)\n",
    "  const candidates: Prompt[] = [{ id: uuid(), instruction: baseInstruction }];\n",
    "  for (let i = 0; i < N; i++) {\n",
    "    const { instruction } = await paraphraseInstruction(config, baseInstruction, meter);\n",
    "    candidates.push({ id: uuid(), instruction });\n",
    "  }\n",
    "\n",
    "  // Evaluate each candidate on the dataset + examples\n",
    "  let best = candidates[0];\n",
    "  let bestScore = -1;\n",
    "\n",
    "  // Loop through candidates and evaluate\n",
    "  for (const p of candidates) {\n",
    "    let sum = 0;\n",
    "\n",
    "    // Loop through eval examples\n",
    "    for (let i = 0; i < data.length; i++) {\n",
    "      const { score, tokens } = await evalExample(p.instruction, data[i]);\n",
    "      meter.add(tokens); // update total tokens\n",
    "      sum += score;\n",
    "    }\n",
    "    const avg = sum / Math.max(1, data.length); // average score over examples\n",
    "    if (avg > bestScore) {\n",
    "      best = p; bestScore = avg; // update best prompt if improved\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Return best prompt, its score, and token usage\n",
    "  const bestPrompts: BestPrompt[] = [{ best, score: bestScore, tokens: meter.snapshot() }];\n",
    "  return { best, bestPrompts, meter };\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"npm:zod\";\n",
    "import { chatText, LLMConfig } from \"./callOllama.ts\";\n",
    "\n",
    "export type Prompt = { id: string; instruction: string };\n",
    "export type BestPrompt = { best: Prompt; score: number; tokens: number };\n",
    "export type EvalExample<E> = (\n",
    "  instruction: string,\n",
    "  ex: E\n",
    ") => Promise<{ score: number; tokens: number }>;\n",
    "\n",
    "export class TokenMeter {\n",
    "  total = 0;\n",
    "  add(n: number) { this.total += Math.max(0, n|0); }\n",
    "  snapshot() { return this.total; }\n",
    "  can(budget?: number | null) { return budget == null || this.total < budget; }\n",
    "}\n",
    "function uuid() { return crypto.randomUUID(); }\n",
    "function pick<T>(xs: T[]) { return xs[Math.floor(Math.random() * xs.length)]; }\n",
    "\n",
    "// ---- Mutators (LLM rewrites of the instruction) ----\n",
    "const MutSchema = z.object({ instruction: z.string().min(8).max(8000) });\n",
    "\n",
    "export type Mutator = (\n",
    "  config: LLMConfig,\n",
    "  parentInstruction: string,\n",
    "  meter: TokenMeter\n",
    ") => Promise<{ instruction: string; tokens: number }>;\n",
    "\n",
    "// Create a mutator using meta prompting\n",
    "function makeMutator(title: string, guidance: string): Mutator {\n",
    "  return async (config, parent, meter) => {\n",
    "    const system =\n",
    "      \"You rewrite prompts. Preserve intent and output schema. Apply the guidance. No examples; be concise.\";\n",
    "    const user =\n",
    "      `GUIDANCE: ${guidance}\\n` +\n",
    "      `Rewrite the instruction below. Keep the same JSON schema and constraints.\\n---\\n${parent}\\n---\\n` +\n",
    "      `Return JSON: {\"instruction\":\"<rewritten>\"}`;\n",
    "    const { response, tokens } = await chatText(\n",
    "      config,\n",
    "      [{ role: \"system\", content: system }, { role: \"user\", content: user }]\n",
    "    );\n",
    "    const instruction = response.trim() || parent;\n",
    "    meter.add(tokens ?? 0);\n",
    "    return { instruction, tokens: tokens ?? 0 };\n",
    "  };\n",
    "}\n",
    "\n",
    "// A few defaults inspired by Ape + PromptBreeder paper\n",
    "export const DEFAULT_MUTATORS: Mutator[] = [\n",
    "  makeMutator(\"format_strict\", \"Stress: output ONLY valid JSON; no explanations.\"),\n",
    "  makeMutator(\"tie_break\", \"If multiple labels seem plausible, choose the MOST SPECIFIC according to the class set.\"),\n",
    "  makeMutator(\"cautious\", \"Prefer safety and common-sense plausibility when unsure.\"),\n",
    "  makeMutator(\"reason_silent\", \"Think step-by-step INTERNALLY and NEVER reveal your reasoning.\"),\n",
    "];\n",
    "\n",
    "type FitnessCacheKey = string;\n",
    "\n",
    "export type EvoOptions<E> = {\n",
    "  config: LLMConfig;\n",
    "  seeds: string[]; // Original prompts to evolve from\n",
    "  data: E[]; // Eval examples for: PIQA, HellaSwag, BoolQ, GSM8K\n",
    "  evalExample: EvalExample<E>;\n",
    "  budget: number | null; // hard token budget (counts mutators + eval)\n",
    "  mutators?: Mutator[]; // default provided\n",
    "};\n",
    "\n",
    "export async function evoOptimize<E>(opts: EvoOptions<E>) {\n",
    "  const { config, seeds, data, evalExample, budget, mutators = DEFAULT_MUTATORS } = opts;\n",
    "  const meter = new TokenMeter();\n",
    "\n",
    "  // Initialize set of prompts\n",
    "  const pop: Prompt[] = (seeds.length ? seeds : ['Return only {\"label\":\"A|B\"}'])\n",
    "    .map(s => ({ id: uuid(), instruction: s }));\n",
    "\n",
    "  // Cache fitness results\n",
    "  const fit = new Map<FitnessCacheKey, number>();\n",
    "  const keyOf = (p: Prompt) => p.instruction;\n",
    "\n",
    "  async function fitness(p: Prompt): Promise<number> {\n",
    "    const k = keyOf(p);\n",
    "    if (fit.has(k)) return fit.get(k)!;\n",
    "\n",
    "    let sum = 0;\n",
    "    for (let i = 0; i < data.length; i++) {\n",
    "      const { score, tokens } = await evalExample(p.instruction, data[i]);\n",
    "      meter.add(tokens);\n",
    "      sum += score;\n",
    "\n",
    "      if (!meter.can(budget)) break; // stop early if budget blown during eval\n",
    "    }\n",
    "    const avg = sum / Math.max(1, data.length);\n",
    "    fit.set(k, avg);\n",
    "    return avg;\n",
    "  }\n",
    "\n",
    "  // From original prompts find the best as starting point\n",
    "  let best = pop[0];\n",
    "  let bestScore = await fitness(best);\n",
    "  const bestPrompts: BestPrompt[] = [{ best, score: bestScore, tokens: meter.snapshot() }];\n",
    "\n",
    "  // Tournament loop until budget reached\n",
    "  while (meter.can(budget)) {\n",
    "    const a = pick(pop), b = pick(pop);\n",
    "    const fa = await fitness(a);\n",
    "    if (!meter.can(budget)) break;\n",
    "    const fb = await fitness(b);\n",
    "    if (!meter.can(budget)) break;\n",
    "\n",
    "    const winner = fa >= fb ? a : b;\n",
    "    const loserIdx = pop.findIndex(x => x.id === (fa >= fb ? b.id : a.id));\n",
    "\n",
    "    // replace the loser with a mutated child of the winner\n",
    "    const mut = pick(mutators);\n",
    "    const { instruction: childInst } = await mut(config, winner.instruction, meter);\n",
    "    if (!meter.can(budget)) break;\n",
    "\n",
    "    // Create child prompt with new uuid and replace in population\n",
    "    const child: Prompt = { id: uuid(), instruction: childInst };\n",
    "    pop[loserIdx] = child;\n",
    "\n",
    "    // Evaluate child fitness and update best if improved\n",
    "    const sChild = await fitness(child);\n",
    "    if (!meter.can(budget)) break;\n",
    "\n",
    "    if (sChild > bestScore) { best = child; bestScore = sChild; }\n",
    "    bestPrompts.push({ best, score: bestScore, tokens: meter.snapshot() });\n",
    "  }\n",
    "\n",
    "  return { best, bestPrompts, meter };\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling (NIG PRior/Posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"npm:zod\";\n",
    "import { chatText, LLMConfig } from \"./callOllama.ts\";\n",
    "\n",
    "export type Prompt = { id: string; instruction: string };\n",
    "export type BestPrompts = { best: Prompt; score: number; tokens: number };\n",
    "export type EvalExample<E> = (\n",
    "  instruction: string,\n",
    "  ex: E\n",
    ") => Promise<{ score: number; tokens: number }>;\n",
    "\n",
    "export class TokenMeter {\n",
    "  total = 0;\n",
    "  add(n: number) {\n",
    "    this.total += Math.max(0, n | 0);\n",
    "  }\n",
    "  snapshot() {\n",
    "    return this.total;\n",
    "  }\n",
    "  can(budget?: number | null) {\n",
    "    return budget == null || this.total < budget;\n",
    "  }\n",
    "}\n",
    "function uuid() {\n",
    "  return crypto.randomUUID();\n",
    "}\n",
    "function randint(n: number) {\n",
    "  return Math.floor(Math.random() * n);\n",
    "}\n",
    "\n",
    "// Create a single mutation of a prompt\n",
    "const MutSchema = z.object({ instruction: z.string().min(8).max(8000) });\n",
    "export async function mutateOnce(\n",
    "  config: LLMConfig,\n",
    "  parent: string,\n",
    "  meter: TokenMeter,\n",
    "  guidance = \"Rewrite to be clearer, shorter, and enforce returning ONLY valid JSON.\"\n",
    "): Promise<{ instruction: string; tokens: number }> {\n",
    "  const system =\n",
    "    \"You rewrite prompts. Preserve intent and output schema. No examples; be concise.\";\n",
    "  const user =\n",
    "    `GUIDANCE: ${guidance}\\n` +\n",
    "    `Rewrite the instruction below. Keep the same JSON schema.\\n---\\n${parent}\\n---\\n` +\n",
    "    `Return JSON: {\"instruction\":\"<rewritten>\"}`;\n",
    "  const { response, tokens } = await chatText(\n",
    "    config,\n",
    "    [\n",
    "      { role: \"system\", content: system },\n",
    "      { role: \"user\", content: user },\n",
    "    ]\n",
    "  );\n",
    "  const instruction = response.trim() || parent;\n",
    "  meter.add(tokens ?? 0);\n",
    "  return { instruction, tokens: tokens ?? 0 };\n",
    "}\n",
    "\n",
    "// Normal Inverse Gamma = mean, variance prior/posterior\n",
    "// Prior and posterior are conjugate this means we can simply update hyperparameters as we see data\n",
    "// If they were'nt conjugate this would mean we would need to approximate the posterior to update it\n",
    "type Posterior = {\n",
    "    mu: number; // mean\n",
    "    kappa: number; // inverse spread of mean\n",
    "    alpha: number; // variance shape\n",
    "    beta: number;  // variance scale\n",
    "};\n",
    "\n",
    "// Sample from normal and inverse gamma distributions\n",
    "// mean is derived from variance's stddev\n",
    "function normal(mean: number, variance: number) {\n",
    "  const u = Math.random();\n",
    "  const v = Math.random();\n",
    "  const z = Math.sqrt(-2 * Math.log(u)) * Math.cos(2 * Math.PI * v);\n",
    "  return mean + Math.sqrt(Math.max(variance, 1e-12)) * z;\n",
    "}\n",
    "\n",
    "// Draw sample variance from inverse gamma\n",
    "function invGamma(alpha: number, beta: number) {\n",
    "  // d = shape, c = scale\n",
    "  const d = alpha - 1 / 3;\n",
    "  const c = 1 / Math.sqrt(9 * d);\n",
    "  let v: number;\n",
    "\n",
    "  // while v is less than 0, re-sample from standard normal\n",
    "  do {\n",
    "    const u = Math.random();\n",
    "    const w = Math.random();\n",
    "    const z = Math.sqrt(-2 * Math.log(u)) * Math.cos(2 * Math.PI * w);\n",
    "    // v = (1 + c * z)^3\n",
    "    v = Math.pow(1 + c * z, 3);\n",
    "  } while (v <= 0);\n",
    "\n",
    "  // return beta / (d * v) as sample from InvGamma\n",
    "  const gammaSample = d * v;\n",
    "  return beta / gammaSample;\n",
    "}\n",
    "\n",
    "export type TSOptions<E> = {\n",
    "  config: LLMConfig;\n",
    "  seeds: string[]; // at least one base instruction\n",
    "  data: E[];\n",
    "  evalExample: EvalExample<E>;\n",
    "  budget: number | null; // hard token budget (mutations + eval)\n",
    "  extraArms?: number; // how many mutated arms to add at start (3 for now)\n",
    "  prior?: Posterior; // Normal inverse gamma prior hyperparameters\n",
    "};\n",
    "\n",
    "export async function tsOptimize<E>(opts: TSOptions<E>) {\n",
    "  const { config, seeds, data, evalExample, budget, extraArms = 3 } = opts;\n",
    "  const prior: Posterior = opts.prior ?? {\n",
    "    mu: 0.5, // prior mean score\n",
    "    kappa: 1e-3, // prior strength\n",
    "    alpha: 1.0, // prior shape (flat prior of (1,1) to start)\n",
    "    beta: 1.0, // prior scale\n",
    "  };\n",
    "  const meter = new TokenMeter();\n",
    "\n",
    "  // Initialize candidate prompts\n",
    "  const arms: Prompt[] = (\n",
    "    seeds.length ? seeds : ['Return only {\"label\":\"A|B\"}']\n",
    "  ).map((s) => ({ id: uuid(), instruction: s }));\n",
    "\n",
    "  // Create extra mutated arms\n",
    "  for (let i = 0; i < extraArms; i++) {\n",
    "    const base = arms[0].instruction;\n",
    "    const { instruction } = await mutateOnce(config, base, meter);\n",
    "    if (!meter.can(budget)) break;\n",
    "    arms.push({ id: uuid(), instruction });\n",
    "  }\n",
    "\n",
    "  // Add posterior for each arm\n",
    "  const P = new Map<string, Posterior>();\n",
    "  arms.forEach((a) => P.set(a.id, { ...prior }));\n",
    "\n",
    "  // Track best by posterior mean\n",
    "  let best = arms[0];\n",
    "  let bestMu = P.get(best.id)!.mu;\n",
    "  const bestPrompts: BestPrompts[] = [\n",
    "    { best, score: bestMu, tokens: meter.snapshot() },\n",
    "  ];\n",
    "\n",
    "  // Loop until budget exhausted\n",
    "  while (meter.can(budget)) {\n",
    "    // Thompson sample each arm\n",
    "    let chosen: Prompt | null = null;\n",
    "    let bestTheta = -Infinity;\n",
    "\n",
    "    for (const a of arms) {\n",
    "      const p = P.get(a.id)!;\n",
    "      const sigma2 = invGamma(p.alpha, p.beta);\n",
    "      const theta = normal(p.mu, sigma2 / p.kappa);\n",
    "      if (theta > bestTheta) {\n",
    "        bestTheta = theta;\n",
    "        chosen = a;\n",
    "      }\n",
    "    }\n",
    "\n",
    "    if (!chosen) break;\n",
    "\n",
    "    // Evaluate at random a given example\n",
    "    const idx = randint(data.length);\n",
    "    const { score, tokens } = await evalExample(chosen.instruction, data[idx]);\n",
    "    meter.add(tokens);\n",
    "\n",
    "    // Posterior update after observing score\n",
    "    // More observations == more certain about mean and variance\n",
    "    const post = P.get(chosen.id)!;\n",
    "    const k1 = post.kappa + 1;\n",
    "    const mu1 = (post.kappa * post.mu + score) / k1;\n",
    "    const a1 = post.alpha + 0.5;\n",
    "    const b1 = post.beta + (post.kappa * (score - post.mu) ** 2) / (2 * k1);\n",
    "    P.set(chosen.id, { mu: mu1, kappa: k1, alpha: a1, beta: b1 });\n",
    "\n",
    "    // Track best by posterior mean\n",
    "    // Update best if improved\n",
    "    best = arms.reduce(\n",
    "      (acc, cur) => (P.get(cur.id)!.mu > P.get(acc.id)!.mu ? cur : acc),\n",
    "      best\n",
    "    );\n",
    "    bestMu = P.get(best.id)!.mu;\n",
    "    bestPrompts.push({ best, score: bestMu, tokens: meter.snapshot() });\n",
    "\n",
    "    if (!meter.can(budget)) break;\n",
    "  }\n",
    "\n",
    "  return { best, bestPrompts, meter, posterior: P };\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code\n",
    "For running the code I simply passed the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%deno run --allow-all main.ts --provider openai --model gpt-4o-mini --examples 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations + Reflection in the Python notebook\n",
    "I put the rest in the python notebook as it was easier to just plot the results in matplotlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
