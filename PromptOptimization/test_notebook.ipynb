{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Optimization: Testing and Implementation\n",
    "\n",
    "This notebook tests all components before running the full experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Ollama Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  In Quebec, Canada, the French language is the primary and only officially recognized language in the National Assembly. Interestingly, English-speaking members are allowed to address the assembly in English, but all responses must be in French! This rule reflects Quebec's strong commitment to preserving its French cultural identity.\n"
     ]
    }
   ],
   "source": [
    "// Testing Ollama Connection\n",
    "import ollama from \"npm:ollama\";\n",
    "\n",
    "try {\n",
    "  const response = await ollama.chat({\n",
    "    model: 'mistral:7b',\n",
    "    messages: [{ role: 'user', content: 'Tell me a fun fact about Quebec' }],\n",
    "    stream: false\n",
    "  });\n",
    "  \n",
    "  console.log(\"Response:\", response.message.content);\n",
    "} catch (error) {\n",
    "  console.error(\"Error:\", error.message);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing JSON Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async function chatJSON(config, messages, schema) {\n",
      "  if (config.provider === \"ollama\") {\n",
      "    return chatJSONOllama(config.model, messages, schema);\n",
      "  } else if (config.provider === \"openai\") {\n",
      "    return chatJSONOpenAI(config, messages, schema);\n",
      "  }\n",
      "  throw new Error(`Unsupported LLM provider: ${config.provider}`);\n",
      "}\n",
      "Parsed data: { answer: \u001b[32m\"yes\"\u001b[39m }\n",
      "Raw response: \"{\\\"answer\\\": \\\"yes\\\"}\"\n",
      "Tokens used: \u001b[33m39\u001b[39m\n",
      "Valid: \u001b[33mtrue\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import { z } from \"npm:zod\";\n",
    "import { chatJSON } from \"./callOllama.ts\";\n",
    "\n",
    "console.log(chatJSON.toString());\n",
    "\n",
    "// Yes/No schema\n",
    "const TestSchema = z.object({\n",
    "  answer: z.enum([\"yes\", \"no\"]),\n",
    "  confidence: z.number().min(0).max(1).optional()\n",
    "});\n",
    "\n",
    "const llmConfig ={\n",
    "    provider:\"ollama\",\n",
    "    model:\"mistral:7b\"\n",
    "}\n",
    "\n",
    "// Test structured JSON output\n",
    "const { data, raw, tokens } = await chatJSON(\n",
    "  llmConfig,\n",
    "  [\n",
    "    { role: \"system\", content: \"You answer yes/no questions. Return JSON: {\\\"answer\\\": \\\"yes|no\\\"}\" },\n",
    "    { role: \"user\", content: \"Is the sky blue?\" }\n",
    "  ],\n",
    "  TestSchema\n",
    ");\n",
    "\n",
    "console.log(\"Parsed data:\", data);\n",
    "console.log(\"Raw response:\", raw);\n",
    "console.log(\"Tokens used:\", tokens);\n",
    "console.log(\"Valid:\", data !== null);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing chatJSON with different response formats...\n",
      "\n",
      "Test 1 - Normal response:\n",
      "  Raw: \"{ \\\"answer\\\": \\\"yes\\\" }\"\n",
      "  Parsed: { answer: \u001b[32m\"yes\"\u001b[39m }\n",
      "  Valid: \u001b[33mtrue\u001b[39m\n",
      "\n",
      "Test 2 - Math response:\n",
      "  Raw: \"{\\\"answer\\\": 4}\"\n",
      "  Parsed: { answer: \u001b[33m4\u001b[39m }\n",
      "  Valid: \u001b[33mtrue\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "const TestSchema = z.object({\n",
    "  answer: z.enum([\"yes\", \"no\"])\n",
    "});\n",
    "\n",
    "console.log(\"Testing chatJSON with different response formats...\\n\");\n",
    "const llmConfig ={\n",
    "    provider:\"ollama\",\n",
    "    model:\"mistral:7b\"\n",
    "}\n",
    "// Test 1: Normal case\n",
    "const test1 = await chatJSON(\n",
    "   llmConfig,\n",
    "  [\n",
    "    { role: \"system\", content: \"Answer yes/no questions.\" },\n",
    "    { role: \"user\", content: 'Is water wet? Return valid JSON' }\n",
    "  ],\n",
    "  TestSchema\n",
    ");\n",
    "\n",
    "console.log(\"Test 1 - Normal response:\");\n",
    "console.log(\"  Raw:\", test1.raw);\n",
    "console.log(\"  Parsed:\", test1.data);\n",
    "console.log(\"  Valid:\", test1.data !== null);\n",
    "console.log(\"\");\n",
    "\n",
    "// Test 2: Math question\n",
    "const MathSchema = z.object({\n",
    "  answer: z.number()\n",
    "});\n",
    "\n",
    "const test2 = await chatJSON(\n",
    "  llmConfig,\n",
    "  [\n",
    "    { role: \"system\", content: \"Solve math problems.\" },\n",
    "    { role: \"user\", content: 'What is 2+2? Return valid JSON' }\n",
    "  ],\n",
    "  MathSchema\n",
    ");\n",
    "\n",
    "console.log(\"Test 2 - Math response:\");\n",
    "console.log(\"  Raw:\", test2.raw);\n",
    "console.log(\"  Parsed:\", test2.data);\n",
    "console.log(\"  Valid:\", test2.data !== null);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datsets\n",
      "\n",
      "PIQA loaded: \u001b[33m5\u001b[39m examples\n",
      "Sample: {\n",
      "  question: \u001b[32m\"How do I ready a guinea pig cage for it's new occupants?\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"Provide the guinea pig with a cage full of a few inches of bedding made of ripped paper strips, you will also need to supply it with a water bottle and a food dish.\"\u001b[39m,\n",
      "    \u001b[32m\"Provide the guinea pig with a cage full of a few inches of bedding made of ripped jeans material, you will also need to supply it with a water bottle and a food dish.\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"A\"\u001b[39m\n",
      "}\n",
      "Sample: {\n",
      "  question: \u001b[32m\"dresser\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"replace drawer with bobby pin \"\u001b[39m,\n",
      "    \u001b[32m\"finish, woodgrain with  bobby pin \"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"B\"\u001b[39m\n",
      "}\n",
      "Sample: {\n",
      "  question: \u001b[32m\"To fight Ivan Drago in Rocky for sega master system.\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"Drago isn't in this game because it was released before Rocky IV.\"\u001b[39m,\n",
      "    \u001b[32m\"You have to defeat Apollo Creed and Clubber Lang first.\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"B\"\u001b[39m\n",
      "}\n",
      "Sample: {\n",
      "  question: \u001b[32m\"Make outdoor pillow.\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"Blow into tin can and tie with rubber band.\"\u001b[39m,\n",
      "    \u001b[32m\"Blow into trash bag and tie with rubber band.\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"B\"\u001b[39m\n",
      "}\n",
      "Sample: {\n",
      "  question: \u001b[32m\"ice box\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"will turn into a cooler if you add water to it\"\u001b[39m,\n",
      "    \u001b[32m\"will turn into a cooler if you add soda to it\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"A\"\u001b[39m\n",
      "}\n",
      "HellaSwag loaded: \u001b[33m5\u001b[39m examples\n",
      "Sample: {\n",
      "  question: \u001b[32m\"A man is sitting on a roof. he\"\u001b[39m,\n",
      "  choices: [\n",
      "    \u001b[32m\"is using wrap to wrap a pair of skis.\"\u001b[39m,\n",
      "    \u001b[32m\"is ripping level tiles off.\"\u001b[39m,\n",
      "    \u001b[32m\"is holding a rubik's cube.\"\u001b[39m,\n",
      "    \u001b[32m\"starts pulling up roofing on a roof.\"\u001b[39m\n",
      "  ],\n",
      "  correct: \u001b[32m\"D\"\u001b[39m\n",
      "}\n",
      "BoolQ loaded: \u001b[33m5\u001b[39m examples\n",
      "Sample: {\n",
      "  question: \u001b[32m\"does ethanol take more energy make that produces\"\u001b[39m,\n",
      "  passage: \u001b[32m\"All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\"\u001b[39m,\n",
      "  answer: \u001b[33mfalse\u001b[39m\n",
      "}\n",
      "GSM8K loaded: \u001b[33m5\u001b[39m examples\n",
      "Sample: {\n",
      "  question: \u001b[32m\"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\u001b[39m,\n",
      "  answer: \u001b[33m18\u001b[39m\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { loadPIQA, loadHellaSwag, loadBoolQ, loadGSM8K } from \"./datasets.ts\";\n",
    "\n",
    "// Downlaod w/ python3 download_datasets.py first\n",
    "\n",
    "// Test PIQA loading\n",
    "console.log(\"Loading datsets\\n\");\n",
    "\n",
    "const piqaData = await loadPIQA(5);\n",
    "console.log(\"PIQA loaded:\", piqaData.length, \"examples\");\n",
    "console.log(\"Sample:\", piqaData[0]);\n",
    "console.log(\"Sample:\", piqaData[1]);\n",
    "console.log(\"Sample:\", piqaData[2]);\n",
    "console.log(\"Sample:\", piqaData[3]);\n",
    "console.log(\"Sample:\", piqaData[4]);\n",
    "\n",
    "const hellaData = await loadHellaSwag(5);\n",
    "console.log(\"HellaSwag loaded:\", hellaData.length, \"examples\");\n",
    "console.log(\"Sample:\", hellaData[0]);\n",
    "\n",
    "const boolqData = await loadBoolQ(5);\n",
    "console.log(\"BoolQ loaded:\", boolqData.length, \"examples\");\n",
    "console.log(\"Sample:\", boolqData[0]);\n",
    "\n",
    "const gsmData = await loadGSM8K(5);\n",
    "console.log(\"GSM8K loaded:\", gsmData.length, \"examples\");\n",
    "console.log(\"Sample:\", gsmData[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Multiple Choice, Boolean, and Integer evals\n",
      "\n",
      "Testing PIQA evaluator\n",
      "Result: { score: \u001b[33m1\u001b[39m, tokens: \u001b[33m151\u001b[39m }\n",
      "Score: \u001b[33m1\u001b[39m\n",
      "Tokens: \u001b[33m151\u001b[39m\n",
      "\n",
      "Testing BoolQ evaluator\n",
      "Result: { score: \u001b[33m0\u001b[39m, tokens: \u001b[33m362\u001b[39m }\n",
      "Score: \u001b[33m0\u001b[39m\n",
      "Tokens: \u001b[33m362\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import { \n",
    "  makePIQAEvaluator, \n",
    "  makeHellaSwagEvaluator,\n",
    "  makeBoolQEvaluator,\n",
    "  makeGSM8KEvaluator \n",
    "} from \"./evals.ts\";\n",
    "\n",
    "console.log(\"Testing Multiple Choice, Boolean, and Integer evals\\n\");\n",
    "\n",
    "const llmConfig ={\n",
    "    provider:\"ollama\",\n",
    "    model:\"mistral:7b\"\n",
    "}\n",
    "\n",
    "// Test PIQA evaluator\n",
    "const piqaEval = makePIQAEvaluator(llmConfig);\n",
    "const baseInstruction = 'You are a classifier. You must follow the provided JSON schema.';\n",
    "\n",
    "console.log(\"Testing PIQA evaluator\");\n",
    "const result1 = await piqaEval(baseInstruction, piqaData[0]);\n",
    "console.log(\"Result:\", result1);\n",
    "console.log(\"Score:\", result1.score);\n",
    "console.log(\"Tokens:\", result1.tokens);\n",
    "\n",
    "// Test BoolQ evaluator\n",
    "const boolqEval = makeBoolQEvaluator(llmConfig);\n",
    "const boolqInstruction = 'Answer yes or no based on the passage. Return valid JSON';\n",
    "\n",
    "console.log(\"\\nTesting BoolQ evaluator\");\n",
    "const result2 = await boolqEval(boolqInstruction, boolqData[0]);\n",
    "console.log(\"Result:\", result2);\n",
    "console.log(\"Score:\", result2.score);\n",
    "console.log(\"Tokens:\", result2.tokens);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test TokenMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the token meter for budgeting\n",
      "\n",
      "Initial: \u001b[33m0\u001b[39m\n",
      "Add(100): \u001b[33m100\u001b[39m\n",
      "Add(250): \u001b[33m350\u001b[39m\n",
      "\n",
      "Budget testing:\n",
      "Can spend (budget=500, used=0): \u001b[33mtrue\u001b[39m\n",
      "Can spend (budget=500, used=400): \u001b[33mtrue\u001b[39m\n",
      "Can spend (budget=500, used=550): \u001b[33mfalse\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import { TokenMeter } from \"./ape.ts\";\n",
    "\n",
    "console.log(\"Testing the token meter for budgeting\\n\");\n",
    "\n",
    "const meter = new TokenMeter();\n",
    "console.log(\"Initial:\", meter.snapshot());\n",
    "\n",
    "meter.add(100);\n",
    "console.log(\"Add(100):\", meter.snapshot());\n",
    "\n",
    "meter.add(250);\n",
    "console.log(\"Add(250):\", meter.snapshot());\n",
    "\n",
    "// Test budget checking (from evo.ts)\n",
    "import { TokenMeter as EvoMeter } from \"./evo.ts\";\n",
    "const budgetMeter = new EvoMeter();\n",
    "const BUDGET = 500;\n",
    "\n",
    "console.log(\"\\nBudget testing:\");\n",
    "console.log(\"Can spend (budget=500, used=0):\", budgetMeter.can(BUDGET));\n",
    "\n",
    "budgetMeter.add(400);\n",
    "console.log(\"Can spend (budget=500, used=400):\", budgetMeter.can(BUDGET));\n",
    "\n",
    "budgetMeter.add(150);\n",
    "console.log(\"Can spend (budget=500, used=550):\", budgetMeter.can(BUDGET));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Paraphrasing (APE Component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing APE optimization\n",
      "\n",
      "Original: You classify questions. Return JSON: {\"label\": \"A|B\"}.\n",
      "\n",
      "Generating 3 paraphrases\n",
      "\n",
      "Paraphrase 1: You classify questions. Return JSON: {\"label\": \"A|B\"}.\n",
      "  Tokens used: 131\n",
      "\n",
      "Paraphrase 2: You classify questions. Return JSON: {\"label\": \"A|B\"}.\n",
      "  Tokens used: 131\n",
      "\n",
      "Paraphrase 3: You classify questions. Return JSON: {\"label\": \"A|B\"}.\n",
      "  Tokens used: 131\n",
      "\n",
      "Total tokens for paraphrasing: \u001b[33m393\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import { paraphraseInstruction } from \"./ape.ts\";\n",
    "import { TokenMeter } from \"./ape.ts\";\n",
    "\n",
    "console.log(\"\\nTesting APE optimization\\n\");\n",
    "\n",
    "const llmConfig ={\n",
    "    provider:\"ollama\",\n",
    "    model:\"mistral:7b\"\n",
    "}\n",
    "\n",
    "const meter2 = new TokenMeter();\n",
    "const baseInst = 'You classify questions. Return JSON: {\"label\": \"A|B\"}.';\n",
    "\n",
    "console.log(\"Original:\", baseInst);\n",
    "console.log(\"\\nGenerating 3 paraphrases\\n\");\n",
    "\n",
    "for (let i = 0; i < 3; i++) {\n",
    "  const { instruction, tokens } = await paraphraseInstruction(\n",
    "    llmConfig,\n",
    "    baseInst,\n",
    "    meter2\n",
    "  );\n",
    "  console.log(`Paraphrase ${i+1}: ${instruction}`);\n",
    "  console.log(`  Tokens used: ${tokens}\\n`);\n",
    "}\n",
    "\n",
    "console.log(\"Total tokens for paraphrasing:\", meter2.snapshot());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test APE Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing APE algorithm\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot read properties of undefined (reading 'provider')",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "TypeError: Cannot read properties of undefined (reading 'provider')",
      "    at chatJSON (file:///Users/cassandrehamel/dev/PromptOptimization6067/PromptOptimization/callOllama.ts:25:16)",
      "    at paraphraseInstruction (file:///Users/cassandrehamel/dev/PromptOptimization6067/PromptOptimization/ape.ts:51:34)",
      "    at apeOptimize (file:///Users/cassandrehamel/dev/PromptOptimization6067/PromptOptimization/ape.ts:86:35)",
      "    at <anonymous>:8:25",
      "    at eventLoopTick (ext:core/01_core.js:179:7)"
     ]
    }
   ],
   "source": [
    "import { apeOptimize } from \"./ape.ts\";\n",
    "\n",
    "console.log(\"Testing APE algorithm\\n\");\n",
    "\n",
    "const llmConfig ={\n",
    "    provider:\"ollama\",\n",
    "    model:\"mistral:7b\"\n",
    "}\n",
    "const smallData = piqaData.slice(0, 3); // Just 3 examples\n",
    "const apeResult = await apeOptimize({\n",
    "  llmConfig,\n",
    "  baseInstruction: baseInstruction,\n",
    "  N: 2,  // Only 2 paraphrases for testing\n",
    "  data: smallData,\n",
    "  evalExample: piqaEval\n",
    "});\n",
    "\n",
    "console.log(\"APE completed!\");\n",
    "console.log(\"\\nResults:\");\n",
    "console.log(\"  Best score:\", apeResult.bestPrompts[0].score.toFixed(3));\n",
    "console.log(\"  Tokens used:\", apeResult.bestPrompts[0].tokens);\n",
    "console.log(\"  Best instruction:\", apeResult.best.instruction.slice(0, 100) + \"...\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Evolution Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Evolution algorithm \n",
      "\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Unsupported LLM provider: undefined",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "Error: Unsupported LLM provider: undefined",
      "    at chatJSON (file:///Users/cassandrehamel/dev/PromptOptimization6067/PromptOptimization/callOllama.ts:30:11)",
      "    at file:///Users/cassandrehamel/dev/PromptOptimization6067/PromptOptimization/evals.ts:29:36",
      "    at fitness (file:///Users/cassandrehamel/dev/PromptOptimization6067/PromptOptimization/evo.ts:88:39)",
      "    at evoOptimize (file:///Users/cassandrehamel/dev/PromptOptimization6067/PromptOptimization/evo.ts:101:25)",
      "    at <anonymous>:3:25"
     ]
    }
   ],
   "source": [
    "import { evoOptimize } from \"./evo.ts\";\n",
    "\n",
    "console.log(\"Testing Evolution algorithm \\n\");\n",
    "\n",
    "const evoResult = await evoOptimize({\n",
    "  model: \"gemma3:4b\",\n",
    "  seeds: [baseInstruction],\n",
    "  data: smallData,\n",
    "  evalExample: piqaEval,\n",
    "  budget: 2000  // Small budget for testing\n",
    "});\n",
    "\n",
    "console.log(\"Evolution completed!\");\n",
    "console.log(\"\\nResults:\");\n",
    "console.log(\"Rounds:\", evoResult.bestPrompts.length);\n",
    "console.log(\"Final score:\", evoResult.bestPrompts[evoResult.bestPrompts.length - 1].score.toFixed(3));\n",
    "console.log(\"Tokens used:\", evoResult.meter.snapshot());\n",
    "console.log(\"Best instruction:\", evoResult.best.instruction.slice(0, 100) + \"...\");\n",
    "\n",
    "// Show improvement over time\n",
    "console.log(\"Scores:\");\n",
    "evoResult.bestPrompts.forEach((h, i) => {\n",
    "  console.log(`    Round ${i}: ${h.score.toFixed(3)} (${h.tokens} tokens)`);\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Thompson Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Thompson Sampling 3 arms\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot read properties of undefined (reading 'provider')",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "TypeError: Cannot read properties of undefined (reading 'provider')",
      "    at chatJSON (file:///Users/cassandrehamel/dev/PromptOptimization6067/PromptOptimization/callOllama.ts:25:16)",
      "    at mutateOnce (file:///Users/cassandrehamel/dev/PromptOptimization6067/PromptOptimization/thompson.ts:54:34)",
      "    at tsOptimize (file:///Users/cassandrehamel/dev/PromptOptimization6067/PromptOptimization/thompson.ts:135:35)",
      "    at <anonymous>:7:24"
     ]
    }
   ],
   "source": [
    "import { tsOptimize } from \"./thompson.ts\";\n",
    "\n",
    "console.log(\"Testing Thompson Sampling 3 arms\\n\");\n",
    "const llmConfig ={\n",
    "    provider:\"ollama\",\n",
    "    model:\"mistral:7b\"\n",
    "}\n",
    "const tsResult = await tsOptimize({\n",
    "  llmConfig,\n",
    "  seeds: [baseInstruction],\n",
    "  data: smallData,\n",
    "  evalExample: piqaEval,\n",
    "  budget: 1500,  // Small budget\n",
    "  extraArms: 2   // 2 mutated variants\n",
    "});\n",
    "\n",
    "console.log(\"Thompson Sampling completed!\");\n",
    "console.log(\"\\nResults:\");\n",
    "console.log(\"Pulls:\", tsResult.bestPrompts.length);\n",
    "console.log(\"Final score:\", tsResult.bestPrompts[tsResult.bestPrompts.length - 1].score.toFixed(3));\n",
    "console.log(\"Tokens used:\", tsResult.meter.snapshot());\n",
    "console.log(\"Best instruction:\", tsResult.best.instruction.slice(0, 100) + \"...\");\n",
    "\n",
    "// Show posterior distributions\n",
    "console.log(\"\\n  Posterior distributions (final):\");\n",
    "tsResult.posterior.forEach((post, armId) => {\n",
    "  console.log(`    Arm ${armId.slice(0, 8)}: mean=${post.mu.toFixed(3)}, spread=${post.kappa.toFixed(1)}`);\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Full Experiment (One Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL EXPERIMENT : PIQA Dataset\n",
      "\n",
      "Dataset: PIQA\n",
      "Examples: 30\n",
      "Budget: 5000 tokens\n",
      "Base instruction: You are a classifier for physical commonsense reasoning. Analyze both options ca...\n",
      "\n",
      "Running APE...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot read properties of undefined (reading 'provider')",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "TypeError: Cannot read properties of undefined (reading 'provider')",
      "    at chatJSON (file:///Users/cassandrehamel/dev/PromptOptimization6067/PromptOptimization/callOllama.ts:25:16)",
      "    at paraphraseInstruction (file:///Users/cassandrehamel/dev/PromptOptimization6067/PromptOptimization/ape.ts:51:34)",
      "    at apeOptimize (file:///Users/cassandrehamel/dev/PromptOptimization6067/PromptOptimization/ape.ts:86:35)",
      "    at <anonymous>:12:23",
      "    at eventLoopTick (ext:core/01_core.js:179:7)"
     ]
    }
   ],
   "source": [
    "console.log(\"FULL EXPERIMENT : PIQA Dataset\");\n",
    "\n",
    "const FULL_DATA = await loadPIQA(30);\n",
    "const FULL_BUDGET = 5000;\n",
    "const FULL_INSTRUCTION = \n",
    "  'You are a classifier for physical commonsense reasoning. ' +\n",
    "  'Analyze both options carefully and choose the more practical solution. ' +\n",
    "  'Return ONLY valid JSON: {\"label\": \"A\"} or {\"label\": \"B\"}.';\n",
    "\n",
    "const fullEval = makePIQAEvaluator(MODEL);\n",
    "\n",
    "console.log(`\\nDataset: PIQA`);\n",
    "console.log(`Examples: ${FULL_DATA.length}`);\n",
    "console.log(`Budget: ${FULL_BUDGET} tokens`);\n",
    "console.log(`Base instruction: ${FULL_INSTRUCTION.slice(0, 80)}...\\n`);\n",
    "\n",
    "// Run all three algorithms\n",
    "console.log(\"Running APE...\");\n",
    "const fullAPE = await apeOptimize({\n",
    "  model: MODEL,\n",
    "  baseInstruction: FULL_INSTRUCTION,\n",
    "  N: 5,\n",
    "  data: FULL_DATA,\n",
    "  evalExample: fullEval\n",
    "});\n",
    "console.log(`Score: ${fullAPE.bestPrompts[0].score.toFixed(3)}, Tokens: ${fullAPE.bestPrompts[0].tokens}`);\n",
    "\n",
    "console.log(\"\\nRunning Evolution...\");\n",
    "const fullEvo = await evoOptimize({\n",
    "  model: MODEL,\n",
    "  seeds: [FULL_INSTRUCTION],\n",
    "  data: FULL_DATA,\n",
    "  evalExample: fullEval,\n",
    "  budget: FULL_BUDGET\n",
    "});\n",
    "const evoFinal = fullEvo.bestPrompts[fullEvo.bestPrompts.length - 1];\n",
    "console.log(`Score: ${evoFinal.score.toFixed(3)}, Tokens: ${evoFinal.tokens}, Rounds: ${fullEvo.bestPrompts.length}`);\n",
    "\n",
    "console.log(\"\\nRunning Thompson Sampling...\");\n",
    "const fullTS = await tsOptimize({\n",
    "  model: MODEL,\n",
    "  seeds: [FULL_INSTRUCTION],\n",
    "  data: FULL_DATA,\n",
    "  evalExample: fullEval,\n",
    "  budget: FULL_BUDGET,\n",
    "  extraArms: 3\n",
    "});\n",
    "const tsFinal = fullTS.bestPrompts[fullTS.bestPrompts.length - 1];\n",
    "console.log(`Score: ${tsFinal.score.toFixed(3)}, Tokens: ${tsFinal.tokens}, Pulls: ${fullTS.bestPrompts.length}`);\n",
    "console.log(\"EXPERIMENT COMPLETE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReferenceError",
     "evalue": "fullAPE is not defined",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "ReferenceError: fullAPE is not defined",
      "    at <anonymous>:4:13"
     ]
    }
   ],
   "source": [
    "// Prepare data for plotting\n",
    "const plotData = {\n",
    "  ape: {\n",
    "    tokens: fullAPE.history.map(h => h.tokens),\n",
    "    scores: fullAPE.history.map(h => h.score),\n",
    "    best: fullAPE.best.instruction\n",
    "  },\n",
    "  evo: {\n",
    "    tokens: fullEvo.history.map(h => h.tokens),\n",
    "    scores: fullEvo.history.map(h => h.score),\n",
    "    best: fullEvo.best.instruction\n",
    "  },\n",
    "  ts: {\n",
    "    tokens: fullTS.history.map(h => h.tokens),\n",
    "    scores: fullTS.history.map(h => h.score),\n",
    "    best: fullTS.best.instruction\n",
    "  }\n",
    "};\n",
    "\n",
    "// Save to file\n",
    "await Deno.writeTextFile(\n",
    "  'notebook_results.json',\n",
    "  JSON.stringify(plotData, null, 2)\n",
    ");\n",
    "\n",
    "console.log(\"Results saved to notebook_results.json\");\n",
    "console.log(\"\\nYou can now plot these results using Python/matplotlib or any plotting tool.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Display Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL COMPARISON\n",
      "Algorithm        | Final Score | Tokens Used | Iterations | Efficiency\n"
     ]
    },
    {
     "ename": "ReferenceError",
     "evalue": "fullAPE is not defined",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "ReferenceError: fullAPE is not defined",
      "    at <anonymous>:3:18"
     ]
    }
   ],
   "source": [
    "console.log(\"FINAL COMPARISON\");\n",
    "console.log(\"Algorithm        | Final Score | Tokens Used | Iterations | Efficiency\");\n",
    "\n",
    "const apeScore = fullAPE.history[0].score;\n",
    "const apeTokens = fullAPE.history[0].tokens;\n",
    "console.log(\n",
    "  `APE              | ${apeScore.toFixed(3).padEnd(11)} | ` +\n",
    "  `${apeTokens.toString().padEnd(11)} | ${\"1\".padEnd(10)} | ` +\n",
    "  `${(apeScore / apeTokens * 1000).toFixed(2)} pts/1k tok`\n",
    ");\n",
    "\n",
    "const evoScore = fullEvo.history[fullEvo.history.length - 1].score;\n",
    "const evoTokens = fullEvo.meter.snapshot();\n",
    "console.log(\n",
    "  `Evolution        | ${evoScore.toFixed(3).padEnd(11)} | ` +\n",
    "  `${evoTokens.toString().padEnd(11)} | ${fullEvo.history.length.toString().padEnd(10)} | ` +\n",
    "  `${(evoScore / evoTokens * 1000).toFixed(2)} pts/1k tok`\n",
    ");\n",
    "\n",
    "const tsScore = fullTS.history[fullTS.history.length - 1].score;\n",
    "const tsTokens = fullTS.meter.snapshot();\n",
    "console.log(\n",
    "  `Thompson         | ${tsScore.toFixed(3).padEnd(11)} | ` +\n",
    "  `${tsTokens.toString().padEnd(11)} | ${fullTS.history.length.toString().padEnd(10)} | ` +\n",
    "  `${(tsScore / tsTokens * 1000).toFixed(2)} pts/1k tok`\n",
    ");\n",
    "\n",
    "console.log(\"=\".repeat(80));\n",
    "console.log(\"\\nKey Observations:\");\n",
    "console.log(`  • APE: Single evaluation pass, most expensive per improvement`);\n",
    "console.log(`  • Evolution: ${fullEvo.history.length} rounds of binary tournament`);\n",
    "console.log(`  • Thompson: ${fullTS.history.length} pulls (most iterations per budget)`);\n",
    "console.log(\"\\nBest Instructions:\");\n",
    "console.log(\"\\nAPE:\", plotData.ape.best.slice(0, 120) + \"...\");\n",
    "console.log(\"\\nEvo:\", plotData.evo.best.slice(0, 120) + \"...\");\n",
    "console.log(\"\\nTS:\", plotData.ts.best.slice(0, 120) + \"...\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Create Simple ASCII Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ASCII PLOTS (Token Usage vs Score)\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "ReferenceError",
     "evalue": "plotData is not defined",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "ReferenceError: plotData is not defined",
      "    at <anonymous>:16:11"
     ]
    }
   ],
   "source": [
    "// Simple ASCII visualization\n",
    "function asciiPlot(data: {tokens: number[], scores: number[]}, label: string, width = 60) {\n",
    "  console.log(`\\n${label}:`);\n",
    "  \n",
    "  const maxTokens = Math.max(...data.tokens);\n",
    "  const maxScore = Math.max(...data.scores);\n",
    "  \n",
    "  for (let i = 0; i < data.scores.length; i++) {\n",
    "    const tokenPos = Math.floor((data.tokens[i] / maxTokens) * width);\n",
    "    const scoreBar = Math.floor((data.scores[i] / maxScore) * 20);\n",
    "    const bar = \"█\".repeat(scoreBar) + \"░\".repeat(20 - scoreBar);\n",
    "    console.log(`  ${data.tokens[i].toString().padStart(6)} tok | ${bar} | ${data.scores[i].toFixed(3)}`);\n",
    "  }\n",
    "}\n",
    "\n",
    "console.log(\"\\n\" + \"=\".repeat(80));\n",
    "console.log(\"ASCII PLOTS (Token Usage vs Score)\");\n",
    "console.log(\"=\".repeat(80));\n",
    "\n",
    "asciiPlot(plotData.ape, \"APE\");\n",
    "asciiPlot(plotData.evo, \"Evolution (first 10 rounds)\", 60);\n",
    "asciiPlot(plotData.ts, \"Thompson Sampling (first 10 pulls)\", 60);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
