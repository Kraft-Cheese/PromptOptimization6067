{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Optimization Results Visualization\n",
    "\n",
    "This notebook visualizes the results from the three prompt optimization algorithms:\n",
    "- **APE**: Automatic Prompt Engineering (paraphrasing)\n",
    "  In which candidate prompts are paraphrased N times from a base prompt using different mutators and evaluated over\n",
    "- **Evolution**: Binary Tournament Genetic Algorithm\n",
    "  In which the pool of candidate prompts is generated using different mutators and in a tournament the winning prompt is mutated and replaces the losing prompt\n",
    "- **Thompson Sampling**: NIG Multi-Armed Bandit\n",
    "  In which candidates are randomly sampled from a distribution of unknown variance and unknown mean, here the candidates are pulled one at a time and evaluaed, the evaluation updates the posteriors\n",
    "\n",
    "\n",
    "Dataset benchmarks: PIQA, HellaSwag, BoolQ, GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(dataset_name):\n",
    "    \"\"\"Load results for a specific dataset\"\"\"\n",
    "    try:\n",
    "        with open(f'results_{dataset_name}.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: results_{dataset_name}.json not found\")\n",
    "        return None\n",
    "\n",
    "# Load all datasets\n",
    "datasets = ['piqa', 'hellaswag', 'boolq', 'gsm8k']\n",
    "results = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    data = load_results(dataset)\n",
    "    if data:\n",
    "        results[dataset] = data\n",
    "        print(f\"Loaded {dataset}: {data['numExamples']} examples\")\n",
    "\n",
    "print(f\"\\nLoaded {len(results)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_history(algorithm_data):\n",
    "    \"\"\"Extract tokens and scores from algorithm results\"\"\"\n",
    "    history = algorithm_data.get('bestPrompts', [])\n",
    "    if not history:\n",
    "        return [], []\n",
    "\n",
    "    tokens = [h['tokens'] for h in history]\n",
    "    scores = [h['score'] for h in history]\n",
    "    return tokens, scores\n",
    "\n",
    "def calculate_efficiency(tokens, score):\n",
    "    \"\"\"Calculate score per 1000 tokens\"\"\"\n",
    "    return (score / tokens * 1000) if tokens > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Individual Dataset Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset_comparison(dataset_name, data):\n",
    "    \"\"\"Create comparison plot for a single dataset\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'Prompt Optimization Results: {dataset_name.upper()}',\n",
    "                 fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Extract data for each algorithm\n",
    "    algorithms = ['ape', 'evo', 'ts']\n",
    "    colors = {'ape': '#e74c3c', 'evo': '#3498db', 'ts': '#2ecc71'}\n",
    "    labels = {'ape': 'APE', 'evo': 'Evolution', 'ts': 'Thompson Sampling'}\n",
    "\n",
    "    histories = {}\n",
    "    for alg in algorithms:\n",
    "        if alg in data:\n",
    "            tokens, scores = extract_history(data[alg])\n",
    "            histories[alg] = {'tokens': tokens, 'scores': scores}\n",
    "\n",
    "    # Score vs Tokens\n",
    "    ax1 = axes[0, 0]\n",
    "    for alg in algorithms:\n",
    "        if alg in histories:\n",
    "            h = histories[alg]\n",
    "            ax1.plot(h['tokens'], h['scores'],\n",
    "                    marker='o', label=labels[alg],\n",
    "                    color=colors[alg], linewidth=2, markersize=4)\n",
    "\n",
    "    ax1.set_xlabel('Tokens Used')\n",
    "    ax1.set_ylabel('Best Score')\n",
    "    ax1.set_title('Score vs Token Usage')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Final scores bar chart\n",
    "    ax2 = axes[0, 1]\n",
    "    final_scores = {}\n",
    "    for alg in algorithms:\n",
    "        if alg in histories and histories[alg]['scores']:\n",
    "            final_scores[labels[alg]] = histories[alg]['scores'][-1]\n",
    "\n",
    "    bars = ax2.bar(final_scores.keys(), final_scores.values(),\n",
    "                   color=[colors[alg] for alg in algorithms if alg in histories])\n",
    "    ax2.set_ylabel('Final Score')\n",
    "    ax2.set_title('Final Performance')\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    # Value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "    # Token usage\n",
    "    ax3 = axes[1, 0]\n",
    "    token_usage = {}\n",
    "    for alg in algorithms:\n",
    "        if alg in histories and histories[alg]['tokens']:\n",
    "            token_usage[labels[alg]] = histories[alg]['tokens'][-1]\n",
    "\n",
    "    ax3.bar(token_usage.keys(), token_usage.values(),\n",
    "           color=[colors[alg] for alg in algorithms if alg in histories])\n",
    "    ax3.set_ylabel('Total Tokens')\n",
    "    ax3.set_title('Token Consumption')\n",
    "\n",
    "    # Efficiency (score per 1k tokens)\n",
    "    ax4 = axes[1, 1]\n",
    "    efficiency = {}\n",
    "    for alg in algorithms:\n",
    "        if alg in histories and histories[alg]['scores'] and histories[alg]['tokens']:\n",
    "            score = histories[alg]['scores'][-1]\n",
    "            tokens = histories[alg]['tokens'][-1]\n",
    "            efficiency[labels[alg]] = calculate_efficiency(tokens, score)\n",
    "\n",
    "    bars = ax4.bar(efficiency.keys(), efficiency.values(),\n",
    "                   color=[colors[alg] for alg in algorithms if alg in histories])\n",
    "    ax4.set_ylabel('Score per 1K Tokens')\n",
    "    ax4.set_title('Efficiency')\n",
    "\n",
    "    # Value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots_{dataset_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "for dataset_name, data in results.items():\n",
    "    plot_dataset_comparison(dataset_name, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Dataset Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cross_dataset_comparison(results_dict):\n",
    "    \"\"\"Compare algorithm performance across all datasets\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Cross-Dataset Algorithm Comparison',\n",
    "                 fontsize=18, fontweight='bold')\n",
    "\n",
    "    algorithms = ['ape', 'evo', 'ts']\n",
    "    colors = {'ape': '#e74c3c', 'evo': '#3498db', 'ts': '#2ecc71'}\n",
    "    labels = {'ape': 'APE', 'evo': 'Evolution', 'ts': 'Thompson'}\n",
    "\n",
    "    dataset_names = list(results_dict.keys())\n",
    "\n",
    "    # Final scores heatmap\n",
    "    ax1 = axes[0, 0]\n",
    "    score_matrix = []\n",
    "    for alg in algorithms:\n",
    "        alg_scores = []\n",
    "        for dataset in dataset_names:\n",
    "            data = results_dict[dataset]\n",
    "            if alg in data and data[alg]['bestPrompts']:\n",
    "                score = data[alg]['bestPrompts'][-1]['score']\n",
    "            else:\n",
    "                score = 0\n",
    "            alg_scores.append(score)\n",
    "        score_matrix.append(alg_scores)\n",
    "\n",
    "    im = ax1.imshow(score_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    ax1.set_xticks(range(len(dataset_names)))\n",
    "    ax1.set_xticklabels([d.upper() for d in dataset_names])\n",
    "    ax1.set_yticks(range(len(algorithms)))\n",
    "    ax1.set_yticklabels([labels[alg] for alg in algorithms])\n",
    "    ax1.set_title('Final Scores Heatmap')\n",
    "\n",
    "    # Add annotations\n",
    "    for i, alg in enumerate(algorithms):\n",
    "        for j, dataset in enumerate(dataset_names):\n",
    "            text = ax1.text(j, i, f'{score_matrix[i][j]:.3f}',\n",
    "                          ha='center', va='center', color='black')\n",
    "\n",
    "    plt.colorbar(im, ax=ax1)\n",
    "\n",
    "    # Token efficiency comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    x = np.arange(len(dataset_names))\n",
    "    width = 0.25\n",
    "\n",
    "    for i, alg in enumerate(algorithms):\n",
    "        efficiencies = []\n",
    "        for dataset in dataset_names:\n",
    "            data = results_dict[dataset]\n",
    "            if alg in data and data[alg]['bestPrompts']:\n",
    "                bp = data[alg]['bestPrompts'][-1]\n",
    "                eff = calculate_efficiency(bp['tokens'], bp['score'])\n",
    "            else:\n",
    "                eff = 0\n",
    "            efficiencies.append(eff)\n",
    "\n",
    "        ax2.bar(x + i*width, efficiencies, width,\n",
    "               label=labels[alg], color=colors[alg])\n",
    "\n",
    "    ax2.set_xlabel('Dataset')\n",
    "    ax2.set_ylabel('Efficiency (Score/1K tokens)')\n",
    "    ax2.set_title('Token Efficiency by Dataset')\n",
    "    ax2.set_xticks(x + width)\n",
    "    ax2.set_xticklabels([d.upper() for d in dataset_names])\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Iterations/rounds comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    for i, alg in enumerate(algorithms):\n",
    "        iterations = []\n",
    "        for dataset in dataset_names:\n",
    "            data = results_dict[dataset]\n",
    "            if alg in data and data[alg]['bestPrompts']:\n",
    "                iters = len(data[alg]['bestPrompts'])\n",
    "            else:\n",
    "                iters = 0\n",
    "            iterations.append(iters)\n",
    "\n",
    "        ax3.bar(x + i*width, iterations, width,\n",
    "               label=labels[alg], color=colors[alg])\n",
    "\n",
    "    ax3.set_xlabel('Dataset')\n",
    "    ax3.set_ylabel('Number of Iterations')\n",
    "    ax3.set_title('Iterations by Algorithm')\n",
    "    ax3.set_xticks(x + width)\n",
    "    ax3.set_xticklabels([d.upper() for d in dataset_names])\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Win rate (best algorithm per dataset)\n",
    "    ax4 = axes[1, 1]\n",
    "    wins = {alg: 0 for alg in algorithms}\n",
    "\n",
    "    for dataset in dataset_names:\n",
    "        data = results_dict[dataset]\n",
    "        best_alg = None\n",
    "        best_score = -1\n",
    "\n",
    "        for alg in algorithms:\n",
    "            if alg in data and data[alg]['bestPrompts']:\n",
    "                score = data[alg]['bestPrompts'][-1]['score']\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_alg = alg\n",
    "\n",
    "        if best_alg:\n",
    "            wins[best_alg] += 1\n",
    "\n",
    "    ax4.pie([wins[alg] for alg in algorithms],\n",
    "           labels=[labels[alg] for alg in algorithms],\n",
    "           colors=[colors[alg] for alg in algorithms],\n",
    "           autopct='%1.0f%%',\n",
    "           startangle=90)\n",
    "    ax4.set_title('Win Rate (Best Performance per Dataset)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots_cross_dataset.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "if results:\n",
    "    plot_cross_dataset_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_table(results_dict):\n",
    "    \"\"\"Create comprehensive summary table\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for dataset_name, data in results_dict.items():\n",
    "        for alg in ['ape', 'evo', 'ts']:\n",
    "            if alg not in data or not data[alg]['bestPrompts']:\n",
    "                continue\n",
    "\n",
    "            bp = data[alg]['bestPrompts'][-1]\n",
    "\n",
    "            rows.append({\n",
    "                'Dataset': dataset_name.upper(),\n",
    "                'Algorithm': {'ape': 'APE', 'evo': 'Evolution', 'ts': 'Thompson'}[alg],\n",
    "                'Final Score': bp['score'],\n",
    "                'Tokens Used': bp['tokens'],\n",
    "                'Iterations': len(data[alg]['bestPrompts']),\n",
    "                'Efficiency': calculate_efficiency(bp['tokens'], bp['score'])\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv('summary_statistics.csv', index=False)\n",
    "    return df\n",
    "\n",
    "summary_df = create_summary_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Prompts Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_best_prompts(results_dict):\n",
    "    \"\"\"Display the best performing prompts\"\"\"\n",
    "    print(\"BEST PROMPTS BY ALGORITHM AND DATASET\")\n",
    "\n",
    "    for dataset_name, data in results_dict.items():\n",
    "        print(f\"Dataset: {dataset_name.upper()}\")\n",
    "\n",
    "        for alg in ['ape', 'evo', 'ts']:\n",
    "            alg_name = {'ape': 'APE', 'evo': 'Evolution', 'ts': 'Thompson'}[alg]\n",
    "\n",
    "            if alg in data and 'best' in data[alg]:\n",
    "                best = data[alg]['best']\n",
    "                score = data[alg]['bestPrompts'][-1]['score'] if data[alg]['bestPrompts'] else 0\n",
    "\n",
    "                print(f\"\\n{alg_name} (Score: {score:.3f}):\")\n",
    "                print(f\"  {best['instruction'][:150]}\")\n",
    "                print()\n",
    "\n",
    "display_best_prompts(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(results_dict):\n",
    "    \"\"\"Learning curves for Evolution and Thompson\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Score Progression Over Time',\n",
    "                 fontsize=16, fontweight='bold')\n",
    "\n",
    "    datasets = list(results_dict.keys())\n",
    "    colors = {'evo': '#3498db', 'ts': '#2ecc71'}\n",
    "\n",
    "    for idx, dataset in enumerate(datasets[:4]):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        data = results_dict[dataset]\n",
    "\n",
    "        for alg in ['evo', 'ts']:\n",
    "            if alg in data and data[alg]['bestPrompts']:\n",
    "                iterations = range(len(data[alg]['bestPrompts']))\n",
    "                scores = [bp['score'] for bp in data[alg]['bestPrompts']]\n",
    "\n",
    "                label = 'Evolution' if alg == 'evo' else 'Thompson'\n",
    "                ax.plot(iterations, scores,\n",
    "                       marker='o', label=label,\n",
    "                       color=colors[alg], linewidth=2, markersize=3)\n",
    "\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Best Score')\n",
    "        ax.set_title(f'{dataset.upper()}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis(summary_df):\n",
    "    \"\"\"Perform statistical analysis on results\"\"\"\n",
    "    print(\"STATISTICAL ANALYSIS\")\n",
    "\n",
    "    # Group by algorithm\n",
    "    grouped = summary_df.groupby('Algorithm')\n",
    "\n",
    "    print(\"\\nMean Performance by Algorithm:\")\n",
    "    print(grouped[['Final Score', 'Tokens Used', 'Efficiency']].mean())\n",
    "\n",
    "    print(\"\\nStandard Deviation:\")\n",
    "    print(grouped[['Final Score', 'Tokens Used', 'Efficiency']].std())\n",
    "\n",
    "    print(\"\\nMedian Performance:\")\n",
    "    print(grouped[['Final Score', 'Tokens Used', 'Efficiency']].median())\n",
    "\n",
    "    print(\"\\nBest Algorithm by Metric:\")\n",
    "    print(f\"  Highest Average Score: {summary_df.groupby('Algorithm')['Final Score'].mean().idxmax()}\")\n",
    "    print(f\"  Lowest Token Usage: {summary_df.groupby('Algorithm')['Tokens Used'].mean().idxmin()}\")\n",
    "    print(f\"  Highest Efficiency: {summary_df.groupby('Algorithm')['Efficiency'].mean().idxmax()}\")\n",
    "\n",
    "statistical_analysis(summary_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
